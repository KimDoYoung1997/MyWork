"""
Performance evaluation and result reporting module
This module provides functions to evaluate and report simulation performance metrics
"""

import numpy as np
from typing import Dict, List, Any

def evaluate_simulation_performance(additional_metrics: Dict[str, List[float]], 
                                  simulation_dt: float, 
                                  control_decimation: int) -> Dict[str, Any]:
    """Simulation performance evaluation based on commands.py metrics
    
    Args:
        additional_metrics: Dictionary containing performance metrics
        simulation_dt: Simulation timestep
        control_decimation: Control decimation factor
        
    Returns:
        Dictionary containing evaluated performance metrics
    """
    # commands.py ê¸°ë°˜ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    avg_anchor_body_pos_error = np.mean(additional_metrics['error_anchor_body_pos'])
    avg_anchor_body_rot_error = np.mean(additional_metrics['error_anchor_body_rot'])
    avg_joint_pos_error = np.mean(additional_metrics['error_joint_pos'])
    avg_joint_vel_error = np.mean(additional_metrics['error_joint_vel'])
    
    max_anchor_body_pos_error = np.max(additional_metrics['error_anchor_body_pos'])
    max_anchor_body_rot_error = np.max(additional_metrics['error_anchor_body_rot'])
    
    # ë°”ë”” ë¶€ìœ„ ì„±ëŠ¥ (ìˆëŠ” ê²½ìš°)
    body_performance = {}
    if 'error_non_anchor_body_pos' in additional_metrics and additional_metrics['error_non_anchor_body_pos']:
        body_performance['avg_body_pos_error'] = np.mean(additional_metrics['error_non_anchor_body_pos'])
        body_performance['avg_body_rot_error'] = np.mean(additional_metrics['error_non_anchor_body_rot'])
    
    # ì‹œë®¬ë ˆì´ì…˜ í†µê³„
    total_steps = len(additional_metrics['error_anchor_body_pos'])
    simulation_time = total_steps * simulation_dt
    policy_frequency = 1 / (simulation_dt * control_decimation)
    
    return {
        'avg_anchor_pos_error': avg_anchor_body_pos_error,
        'avg_anchor_rot_error': avg_anchor_body_rot_error,
        'avg_joint_pos_error': avg_joint_pos_error,
        'avg_joint_vel_error': avg_joint_vel_error,
        'max_anchor_pos_error': max_anchor_body_pos_error,
        'max_anchor_rot_error': max_anchor_body_rot_error,
        'total_steps': total_steps,
        'simulation_time': simulation_time,
        'policy_frequency': policy_frequency,
        **body_performance
    }

def print_performance_report(performance_metrics: Dict[str, float], 
                           additional_metrics: Dict[str, List[float]]) -> None:
    """Print comprehensive performance report
    
    Args:
        performance_metrics: Evaluated performance metrics
        additional_metrics: Raw performance metrics for body parts
    """
    print(f"commands.py ê¸°ë°˜ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   Anchor Position Error: {performance_metrics['avg_anchor_pos_error']:.4f} m (ìµœëŒ€: {performance_metrics['max_anchor_pos_error']:.4f} m)")
    print(f"   Anchor Rotation Error: {performance_metrics['avg_anchor_rot_error']:.4f} rad (ìµœëŒ€: {performance_metrics['max_anchor_rot_error']:.4f} rad)")
    print(f"   Joint Position Error: {performance_metrics['avg_joint_pos_error']:.4f} rad")
    print(f"   Joint Velocity Error: {performance_metrics['avg_joint_vel_error']:.4f} rad/s")
    
    # ë°”ë”” ë¶€ìœ„ ì„±ëŠ¥ (ìˆëŠ” ê²½ìš°)
    if 'avg_body_pos_error' in performance_metrics:
        print(f"\nBody Part Performance:")
        print(f"   Body Position Error: {performance_metrics['avg_body_pos_error']:.4f} m")
        print(f"   Body Rotation Error: {performance_metrics['avg_body_rot_error']:.4f} rad")
    
    print(f"\nSim-to-Sim ì‹¤í–‰ í†µê³„:")
    print(f"   ì´ ì²˜ë¦¬ëœ ìŠ¤í…: {performance_metrics['total_steps']}")
    print(f"   ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {performance_metrics['simulation_time']:.2f}ì´ˆ")
    print(f"   ì •ì±… ì¶”ë¡  ì£¼íŒŒìˆ˜: {performance_metrics['policy_frequency']:.1f}Hz")

def evaluate_sim2sim_success(performance_metrics: Dict[str, float]) -> str:
    """Evaluate Sim-to-Sim success level based on performance metrics
    
    Args:
        performance_metrics: Evaluated performance metrics
        
    Returns:
        Success level string
    """
    avg_anchor_pos_error = performance_metrics['avg_anchor_pos_error']
    avg_anchor_rot_error = performance_metrics['avg_anchor_rot_error']
    
    if avg_anchor_pos_error < 0.01 and avg_anchor_rot_error < 0.1:
        return "excellent"
    elif avg_anchor_pos_error < 0.05 and avg_anchor_rot_error < 0.3:
        return "good"
    else:
        return "needs_improvement"

def print_sim2sim_success_report(success_level: str) -> None:
    """Print Sim-to-Sim success report based on success level
    
    Args:
        success_level: Success level string
    """
    if success_level == "excellent":
        print("\nğŸ‰ Sim-to-Sim ì„±ê³µë„: ìš°ìˆ˜ (Excellent)")
        print("   Beyond Mimic ë°©ë²•ë¡ ì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   Isaac Lab â†’ MuJoCo ì „í™˜ì´ ë§¤ìš° ì •í™•í•˜ê²Œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif success_level == "good":
        print("\n Sim-to-Sim ì„±ê³µë„: ì–‘í˜¸ (Good)")
        print("   ëª¨ì…˜ íŠ¸ë˜í‚¹ì´ ì˜ ìˆ˜í–‰ë˜ê³  ìˆì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        print("   ì¢Œí‘œê³„ ë³€í™˜ ì—†ì´ë„ ìƒë‹¹í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸  Sim-to-Sim ì„±ê³µë„: ê°œì„  í•„ìš” (Needs Improvement)")
        print("   ì •ì±… íŠœë‹ì´ë‚˜ í•™ìŠµ ë°ì´í„° ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   ì•µì»¤ë§ ë©”ì»¤ë‹ˆì¦˜ì´ë‚˜ ê´€ì°°ê°’ êµ¬ì„± ì¬ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")

def generate_final_performance_report(additional_metrics: Dict[str, List[float]], 
                                   simulation_dt: float, 
                                   control_decimation: int) -> tuple:
    """Generate and print complete performance report
    
    Args:
        additional_metrics: Raw performance metrics
        simulation_dt: Simulation timestep
        control_decimation: Control decimation factor
    """
    # ì„±ëŠ¥ ì§€í‘œ í‰ê°€
    performance_metrics = evaluate_simulation_performance(
        additional_metrics, simulation_dt, control_decimation
    )
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥
    print_performance_report(performance_metrics, additional_metrics)
    
    # Sim-to-Sim ì„±ê³µë„ í‰ê°€ ë° ì¶œë ¥
    success_level = evaluate_sim2sim_success(performance_metrics)
    print_sim2sim_success_report(success_level)
    
    return performance_metrics, success_level
